# üìä **Observability Guide**
## *LOCAL-PLUS Monitoring, Logging, Tracing & APM*

---

> **Retour vers** : [Architecture Overview](../EntrepriseArchitecture.md)

---

# üìã **Table of Contents**

1. [Stack Overview](#stack-overview)
2. [Telemetry Pipeline](#telemetry-pipeline)
3. [Metrics (Prometheus)](#metrics-prometheus)
4. [Logs (Loki)](#logs-loki)
5. [Traces (Tempo)](#traces-tempo)
6. [APM (Application Performance Monitoring)](#apm-application-performance-monitoring)
7. [Cardinality Management](#cardinality-management)
8. [SLI/SLO/Error Budgets](#slisloerror-budgets)
9. [Alerting Strategy](#alerting-strategy)
10. [Dashboards & Visualizations](#dashboards--visualizations)

---

# üèóÔ∏è **Stack Overview**

## Self-Hosted Stack (Co√ªt Minimal)

| Composant | Outil | Co√ªt | Retention |
|-----------|-------|------|-----------|
| **Metrics** | Prometheus | 0‚Ç¨ (self-hosted) | 15 jours local |
| **Metrics long-term** | Prometheus avec Remote Write ‚Üí S3 | ~5‚Ç¨/mois S3 | 1 an |
| **Logs** | Loki | 0‚Ç¨ (self-hosted) | 30 jours (GDPR) |
| **Traces** | Tempo | 0‚Ç¨ (self-hosted) | 7 jours |
| **Dashboards** | Grafana | 0‚Ç¨ (self-hosted) | N/A |
| **Fallback logs** | CloudWatch Logs | Tier gratuit 5GB | 7 jours |

**Co√ªt estim√© : < 50‚Ç¨/mois** (principalement stockage S3)

### Note sur le stockage long-terme

Pour conserver les m√©triques au-del√† de 15 jours :

| Option | Description | Complexit√© |
|--------|-------------|------------|
| **Remote Write vers S3** | Prometheus √©crit directement vers un backend compatible S3 | Simple |
| **Grafana Mimir** | Solution CNCF pour le stockage long-terme, scalable | Moyen |
| **Victoria Metrics** | Alternative performante, compatible Prometheus | Moyen |

> **Choix Local-Plus :** Remote Write vers S3 via Grafana Mimir (ou Victoria Metrics) ‚Äî pas besoin de composants additionnels complexes.

---

# üîÑ **Telemetry Pipeline**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Applications   ‚îÇ     ‚îÇ  OTel Collector ‚îÇ     ‚îÇ   Backends      ‚îÇ
‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ     ‚îÇ                 ‚îÇ
‚îÇ  ‚Ä¢ SDK Python   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  ‚Ä¢ Receivers    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ  ‚Ä¢ Prometheus   ‚îÇ
‚îÇ  ‚Ä¢ Auto-instr   ‚îÇ     ‚îÇ  ‚Ä¢ Processors   ‚îÇ     ‚îÇ  ‚Ä¢ Loki         ‚îÇ
‚îÇ                 ‚îÇ     ‚îÇ  ‚Ä¢ Exporters    ‚îÇ     ‚îÇ  ‚Ä¢ Tempo        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚îÇ
                               ‚îÇ Scrubbing
                               ‚ñº
                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                        ‚îÇ GDPR Compliant  ‚îÇ
                        ‚îÇ ‚Ä¢ No user_id    ‚îÇ
                        ‚îÇ ‚Ä¢ No PII        ‚îÇ
                        ‚îÇ ‚Ä¢ No PAN        ‚îÇ
                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## OTel Collector ‚Äî R√¥le

| Composant | R√¥le | Exemples |
|-----------|------|----------|
| **Receivers** | R√©ceptionne les donn√©es de t√©l√©m√©trie | OTLP (gRPC/HTTP), Prometheus scrape |
| **Processors** | Transforme, filtre, enrichit les donn√©es | Batch, Memory limiter, Attribute deletion (PII), Sampling |
| **Exporters** | Envoie vers les backends | Prometheus, Loki, Tempo |

## GDPR Compliance ‚Äî Donn√©es supprim√©es

| Donn√©e | Action | Raison |
|--------|--------|--------|
| `user.id` | Supprim√© | PII |
| `user.email` | Supprim√© | PII |
| `http.client_ip` | Hash√© | Anonymisation |
| `*_bucket` haute cardinalit√© | Filtr√© | Performance |

---

# üìà **Metrics (Prometheus)**

## Comment Prometheus collecte les m√©triques

Prometheus utilise un mod√®le **pull** : il va chercher les m√©triques sur chaque cible.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                  PROMETHEUS ‚Äî MOD√àLE DE COLLECTE                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                              ‚îÇ
‚îÇ                        PROMETHEUS                                            ‚îÇ
‚îÇ                       (scraping)                                             ‚îÇ
‚îÇ                            ‚îÇ                                                 ‚îÇ
‚îÇ         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                             ‚îÇ
‚îÇ         ‚îÇ                  ‚îÇ                  ‚îÇ                             ‚îÇ
‚îÇ         ‚ñº                  ‚ñº                  ‚ñº                             ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ   ‚îÇ Pod A    ‚îÇ      ‚îÇ Pod B    ‚îÇ      ‚îÇ Pod C    ‚îÇ                         ‚îÇ
‚îÇ   ‚îÇ          ‚îÇ      ‚îÇ          ‚îÇ      ‚îÇ          ‚îÇ                         ‚îÇ
‚îÇ   ‚îÇ :8080    ‚îÇ      ‚îÇ :8080    ‚îÇ      ‚îÇ :9090    ‚îÇ                         ‚îÇ
‚îÇ   ‚îÇ /metrics ‚îÇ      ‚îÇ /metrics ‚îÇ      ‚îÇ /metrics ‚îÇ                         ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îÇ   Prometheus fait GET http://pod:port/metrics toutes les 30s               ‚îÇ
‚îÇ                                                                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## D√©couverte des cibles ‚Äî ServiceMonitor (Prometheus Operator)

Le **Prometheus Operator** utilise des **Custom Resources** pour configurer automatiquement les cibles de scraping.

| Ressource | Ce qu'elle fait |
|-----------|-----------------|
| **ServiceMonitor** | S√©lectionne les Services via labels, Prometheus scrape les pods derri√®re |
| **PodMonitor** | S√©lectionne les Pods directement via labels |

**Flux :**

1. Le d√©veloppeur d√©ploie son service avec un label (ex: `app: svc-ledger`)
2. Un ServiceMonitor s√©lectionne ce label
3. Prometheus Operator configure automatiquement Prometheus
4. Prometheus scrape `/metrics` sur le port sp√©cifi√©

**Avantages :**
- GitOps-friendly ‚Äî fichier s√©par√©, versionn√©, reviewable
- S√©paration des concerns ‚Äî monitoring d√©coupl√© du d√©ploiement
- Flexibilit√© ‚Äî intervalles, relabeling, TLS, authentification

## Endpoints typiques

| Service | Port | Path | Description |
|---------|------|------|-------------|
| **FastAPI (Python)** | 8080 | `/metrics` | Via `prometheus-fastapi-instrumentator` |
| **Go gRPC** | 9090 | `/metrics` | Via `promhttp` handler |
| **Grafana** | 3000 | `/metrics` | M√©triques internes |
| **ArgoCD** | 8083 | `/metrics` | M√©triques application |
| **Node Exporter** | 9100 | `/metrics` | M√©triques syst√®me (CPU, RAM, disk) |

---

# üìù **Logs (Loki)**

## Configuration

| Param√®tre | Valeur | Raison |
|-----------|--------|--------|
| **Retention** | 30 jours | GDPR compliance |
| **Max query series** | 5000 | Protection performance |
| **Max entries per query** | 10000 | Protection performance |
| **Storage backend** | S3 | Co√ªt faible, durabilit√© |

## Log Labels (Low Cardinality)

| Label | Exemple | Cardinalit√© |
|-------|---------|-------------|
| `namespace` | svc-ledger | Low |
| `pod` | svc-ledger-abc123 | Medium |
| `container` | svc-ledger | Low |
| `level` | info, error, warn | Very Low |
| `stream` | stdout, stderr | Very Low |

**‚ö†Ô∏è Never use as labels:** `user_id`, `request_id`, `trace_id`

---

# üîç **Traces (Tempo)**

## Configuration

| Param√®tre | Valeur | Raison |
|-----------|--------|--------|
| **Retention** | 7 jours | Co√ªt / utilit√© |
| **Backend** | S3 | Durabilit√© |
| **Protocol** | OTLP (gRPC + HTTP) | Standard OTel |

## Trace-to-Logs Correlation

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     trace_id     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ     TRACES      ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ      LOGS       ‚îÇ
‚îÇ     (Tempo)     ‚îÇ                  ‚îÇ     (Loki)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                                    ‚îÇ
         ‚îÇ Exemplars (trace_id in metrics)    ‚îÇ
         ‚îÇ                                    ‚îÇ
         ‚ñº                                    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GRAFANA                               ‚îÇ
‚îÇ  ‚Ä¢ Click trace ‚Üí See logs for that request              ‚îÇ
‚îÇ  ‚Ä¢ Click metric spike ‚Üí Jump to exemplar trace          ‚îÇ
‚îÇ  ‚Ä¢ Click error log ‚Üí Navigate to full trace             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

# üéØ **APM (Application Performance Monitoring)**

## Stack APM

| Composant | Outil | Usage |
|-----------|-------|-------|
| **Distributed Tracing** | Tempo + OTel | Request flow, latency breakdown |
| **Profiling** | Pyroscope (Grafana) | CPU/Memory profiling continu |
| **Error Tracking** | Sentry (self-hosted) | Exception tracking, stack traces |
| **Database APM** | pg_stat_statements | Query performance |
| **Real User Monitoring** | Grafana Faro | Frontend performance (si applicable) |

## Sampling Strategy

| Environment | Head Sampling | Tail Sampling | Rationale |
|-------------|---------------|---------------|-----------|
| **Dev** | 100% | N/A | Full visibility pour debug |
| **Staging** | 50% | Errors: 100% | Balance cost/visibility |
| **Prod** | 10% | Errors: 100%, Slow: 100% (>500ms) | Cost optimization |

### Tail Sampling ‚Äî R√®gles

| R√®gle | Condition | Pourquoi |
|-------|-----------|----------|
| **error-policy** | Status = ERROR | Toujours conserver les erreurs |
| **slow-policy** | Latency > 500ms | D√©tecter les lenteurs |
| **probabilistic-policy** | 10% al√©atoire | √âchantillonnage de base |

---

# üìâ **Cardinality Management**

## Label Rules

| Label | Action | Rationale |
|-------|--------|-----------|
| `user_id` | DROP | High cardinality, use traces |
| `request_id` | DROP | Use trace_id instead |
| `http.url` | DROP | URLs uniques = explosion |
| `http.route` | KEEP | Templated, low cardinality |
| `service.name` | KEEP | Essential |
| `http.method` | KEEP | Low cardinality |
| `http.status_code` | KEEP | Low cardinality |

## Cardinality Limits

| Metric Type | Max Labels | Max Series |
|-------------|------------|------------|
| Counter | 5 | 1000 |
| Histogram | 4 | 500 |
| Gauge | 5 | 1000 |

---

# üéØ **SLI/SLO/Error Budgets**

## Service SLOs

| Service | SLI | SLO | Error Budget | Burn Rate Alert |
|---------|-----|-----|--------------|-----------------|
| **svc-ledger** | Availability | 99.9% | 43 min/mois | 14.4x = 1h alert |
| **svc-ledger** | Latency P99 | < 200ms | N/A | P99 > 200ms for 5min |
| **svc-wallet** | Availability | 99.9% | 43 min/mois | 14.4x = 1h alert |
| **Platform** | Availability | 99.5% | 3.6h/mois | 6x = 2h alert |

## SLO Formulas

| M√©trique | Formule | Signification |
|----------|---------|---------------|
| **Availability** | `1 - (erreurs / total)` | % de requ√™tes sans erreur 5xx |
| **Error Budget Remaining** | `1 - ((1 - availability) / (1 - SLO))` | % du budget restant |
| **Burn Rate** | `error_rate / allowed_error_rate` | Vitesse de consommation du budget |

---

# üö® **Alerting Strategy**

## Severity Levels

| Severity | Exemple | Notification | On-call |
|----------|---------|--------------|---------|
| **P1 ‚Äî Critical** | svc-ledger down | PagerDuty immediate | Wake up |
| **P2 ‚Äî High** | Error rate > 5% | Slack + PagerDuty 15min | Within 30min |
| **P3 ‚Äî Medium** | Latency P99 > 500ms | Slack | Business hours |
| **P4 ‚Äî Low** | Disk usage > 80% | Slack | Next day |

## Alertes principales

| Alerte | Condition | S√©v√©rit√© | Action |
|--------|-----------|----------|--------|
| **ServiceDown** | `up == 0` pendant 1min | P1 | Runbook: restart, check logs |
| **HighErrorRate** | Error rate > 5% pendant 5min | P2 | Investigate traces + Sentry |
| **LatencyDegradation** | P99 > 2x baseline pendant 10min | P2 | Check slow spans in Tempo |
| **DiskAlmostFull** | Disk > 80% | P4 | Extend volume or cleanup |

---

# üìä **Dashboards & Visualizations**

## Types de visualisations Grafana par type de m√©trique

### Counter (Compteur)

> **D√©finition :** Valeur qui ne peut qu'augmenter (ou reset √† 0 au restart).

| Visualisation | Query | Quand utiliser |
|---------------|-------|----------------|
| **Stat (nombre)** | `sum(http_requests_total)` | Total absolu |
| **Time Series (rate)** | `rate(http_requests_total[5m])` | D√©bit par seconde (RPS) |
| **Bar Gauge** | `sum by (status_code) (rate(http_requests_total[5m]))` | Comparaison entre labels |

```
Exemple visuel ‚Äî Counter en Time Series (rate)

  RPS
  30 ‚îÇ          ‚ï≠‚îÄ‚îÄ‚îÄ‚ïÆ
     ‚îÇ    ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ   ‚îÇ
  20 ‚îÇ‚îÄ‚îÄ‚îÄ‚ïØ         ‚îÇ
     ‚îÇ             ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ
  10 ‚îÇ                  ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ temps
        10:00   10:05   10:10
```

### Gauge (Jauge)

> **D√©finition :** Valeur instantan√©e qui peut monter ou descendre (temp√©rature, connexions actives, CPU%).

| Visualisation | Query | Quand utiliser |
|---------------|-------|----------------|
| **Gauge (cadran)** | `pg_stat_activity_count` | Valeur courante visuelle |
| **Stat** | `node_memory_MemAvailable_bytes / 1e9` | Valeur simple avec unit√© |
| **Time Series** | `process_resident_memory_bytes` | √âvolution dans le temps |
| **Heatmap** | `avg by (pod) (container_memory_usage_bytes)` | Comparaison multi-pods |

```
Exemple visuel ‚Äî Gauge en cadran

        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ     CPU %       ‚îÇ
        ‚îÇ                 ‚îÇ
        ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
        ‚îÇ   ‚îÇ  67%  ‚îÇ     ‚îÇ
        ‚îÇ   ‚îÇ  ‚ñà‚ñà‚ñà  ‚îÇ     ‚îÇ
        ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
        ‚îÇ 0%         100% ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Histogram (Histogramme)

> **D√©finition :** Distribution de valeurs dans des "buckets" (ex: latence). Permet de calculer des percentiles.

| Visualisation | Query | Quand utiliser |
|---------------|-------|----------------|
| **Time Series (P50/P95/P99)** | `histogram_quantile(0.99, rate(http_request_duration_seconds_bucket[5m]))` | Latency trends |
| **Heatmap** | `sum by (le) (rate(http_request_duration_seconds_bucket[5m]))` | Distribution visuelle |
| **Stat** | `histogram_quantile(0.95, ...)` | Valeur P95 courante |

```
Exemple visuel ‚Äî Histogram en Heatmap (latence)

  Latency
  1s    ‚îÇ‚ñë‚ñë‚ñì‚ñì‚ñë‚ñë
  500ms ‚îÇ‚ñì‚ñì‚ñì‚ñì‚ñì‚ñì‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  200ms ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  100ms ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
  50ms  ‚îÇ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂ temps
           10:00        10:30        11:00
        
  ‚ñë = peu de requ√™tes   ‚ñì = moyen   ‚ñà = beaucoup
```

### Summary

> **D√©finition :** Comme Histogram mais les percentiles sont calcul√©s c√¥t√© client (moins flexible).

| Visualisation | Query | Quand utiliser |
|---------------|-------|----------------|
| **Time Series** | `go_gc_duration_seconds{quantile="0.99"}` | Pr√©-calcul√© |
| **Stat** | `go_gc_duration_seconds{quantile="0.5"}` | M√©diane |

> **Note :** Pr√©f√©rer Histogram. Summary est principalement utilis√© par les exporters Go legacy.

---

## Dashboards recommand√©s par audience

### Dashboard 1 : Service Overview (On-call)

| Panel | Type | M√©trique | Visualisation |
|-------|------|----------|---------------|
| **Request Rate** | Counter | `rate(http_requests_total[5m])` | Time Series |
| **Error Rate %** | Counter | `rate(errors[5m]) / rate(total[5m]) * 100` | Time Series + Threshold |
| **Latency P50/P95/P99** | Histogram | `histogram_quantile(...)` | Time Series (3 lignes) |
| **Active Requests** | Gauge | `http_requests_in_flight` | Stat |

### Dashboard 2 : Infrastructure (Platform Team)

| Panel | Type | M√©trique | Visualisation |
|-------|------|----------|---------------|
| **CPU Usage %** | Gauge | `container_cpu_usage_seconds_total` | Gauge cadran |
| **Memory Usage** | Gauge | `container_memory_usage_bytes` | Bar Gauge |
| **Network I/O** | Counter | `rate(container_network_receive_bytes_total[5m])` | Time Series |
| **Disk Usage %** | Gauge | `node_filesystem_avail_bytes / node_filesystem_size_bytes` | Gauge |
| **Pod Count** | Gauge | `kube_pod_status_phase{phase="Running"}` | Stat |

### Dashboard 3 : Database (Backend Devs)

| Panel | Type | M√©trique | Visualisation |
|-------|------|----------|---------------|
| **Active Connections** | Gauge | `pg_stat_activity_count` | Gauge cadran |
| **Query Duration P95** | Histogram | `pg_stat_statements_mean_time_seconds` | Time Series |
| **Transactions/sec** | Counter | `rate(pg_stat_database_xact_commit[5m])` | Time Series |
| **Replication Lag** | Gauge | `pg_replication_lag_seconds` | Stat avec threshold |
| **Cache Hit Ratio** | Gauge | `pg_stat_database_blks_hit / (blks_hit + blks_read)` | Stat % |

### Dashboard 4 : Business Metrics (Product)

| Panel | Type | M√©trique | Visualisation |
|-------|------|----------|---------------|
| **Transactions Cr√©√©es** | Counter | `sum(rate(ledger_transactions_total[1h]))` | Stat (big number) |
| **Montant Total Trait√©** | Counter | `sum(ledger_amount_processed_total)` | Stat avec unit√© ‚Ç¨ |
| **Wallets Actifs** | Gauge | `wallet_active_count` | Stat |
| **Erreurs M√©tier** | Counter | `sum by (error_type) (rate(business_errors_total[5m]))` | Bar chart |

---

## R√©capitulatif : Quel type pour quelle m√©trique ?

| M√©trique | Type Prometheus | Visualisation Grafana |
|----------|-----------------|----------------------|
| Nombre de requ√™tes | Counter | Time Series (rate) |
| Erreurs totales | Counter | Time Series (rate) + Stat |
| Latence | Histogram | Time Series (quantile) + Heatmap |
| Connexions actives | Gauge | Gauge cadran ou Stat |
| M√©moire utilis√©e | Gauge | Time Series ou Bar Gauge |
| CPU % | Gauge | Gauge cadran |
| Dur√©e GC | Summary | Time Series |
| Taille de queue | Gauge | Stat avec threshold |

---

*Document maintenu par : Platform Team*  
*Derni√®re mise √† jour : Janvier 2026*
