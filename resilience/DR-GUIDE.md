# âš¡ **Resilience & Disaster Recovery Guide**
## *LOCAL-PLUS Backup, Recovery & Business Continuity*

---

> **Retour vers** : [Architecture Overview](../EntrepriseArchitecture.md)  
> **Voir aussi** : [Testing Strategy](../testing/TESTING-STRATEGY.md) â€” Tests de performance et chaos

---

# ğŸ“‹ **Table of Contents**

1. [Failure Modes](#failure-modes)
2. [Backup Strategy](#backup-strategy)
3. [Automated Recovery](#automated-recovery)
4. [Chaos Engineering](#chaos-engineering)
5. [Disaster Recovery](#disaster-recovery)
6. [Business Continuity](#business-continuity)

---

# ğŸ”¥ **Failure Modes**

## Failure Matrix

| Failure | Detection | Recovery | RTO | Impact |
|---------|-----------|----------|-----|--------|
| **Pod crash** | Liveness probe | K8s restart automatique | < 30s | None (replicas) |
| **Node failure** | Node NotReady | Pod reschedule automatique | < 2min | Minor latency spike |
| **AZ failure** | Multi-AZ detect | Traffic shift automatique | < 5min | Reduced capacity |
| **DB primary failure** | Aiven health | Failover automatique | < 5min | Brief connection errors |
| **Kafka broker failure** | Aiven health | Rebalance automatique | < 2min | Brief producer retries |
| **Cache failure** | Health check | Fallback to DB automatique | < 1min | Increased latency |
| **Full region failure** | Health checks | DR procedure | 4h (target) | Extended outage |

## Blast Radius Analysis

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         BLAST RADIUS ANALYSIS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  SINGLE POD FAILURE                                                         â”‚
â”‚  â””â”€â”€ Impact: None (other replicas serve traffic)                           â”‚
â”‚  â””â”€â”€ Recovery: Automatique via Kubernetes                                  â”‚
â”‚                                                                              â”‚
â”‚  SINGLE NODE FAILURE                                                        â”‚
â”‚  â””â”€â”€ Impact: 10-20% capacity loss temporarily                              â”‚
â”‚  â””â”€â”€ Recovery: Automatique via pod anti-affinity + reschedule              â”‚
â”‚                                                                              â”‚
â”‚  SINGLE AZ FAILURE                                                          â”‚
â”‚  â””â”€â”€ Impact: 33% capacity loss                                             â”‚
â”‚  â””â”€â”€ Recovery: Automatique via Multi-AZ + ALB health checks                â”‚
â”‚                                                                              â”‚
â”‚  DATABASE PRIMARY FAILURE                                                   â”‚
â”‚  â””â”€â”€ Impact: Write unavailability ~5 min                                   â”‚
â”‚  â””â”€â”€ Recovery: Automatique via Aiven failover                              â”‚
â”‚                                                                              â”‚
â”‚  FULL REGION FAILURE                                                        â”‚
â”‚  â””â”€â”€ Impact: Complete service unavailability                               â”‚
â”‚  â””â”€â”€ Recovery: Semi-automatique (DR procedure)                             â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ’¾ **Backup Strategy**

## Backup Matrix

| Data | Method | Frequency | Retention | Location | Encryption |
|------|--------|-----------|-----------|----------|------------|
| **PostgreSQL** | Aiven automated | Hourly | 7 jours | Aiven (cross-AZ) | AES-256 |
| **PostgreSQL PITR** | Aiven WAL | Continuous | 24h | Aiven | AES-256 |
| **Kafka** | Topic retention | N/A | 7 jours | Aiven | AES-256 |
| **Valkey** | RDB + AOF | Continuous | 24h | Aiven | AES-256 |
| **Terraform state** | S3 versioning | Every apply | 90 jours | S3 | KMS |
| **Git repos** | GitHub | Every push | Infini | GitHub | At-rest |
| **Secrets (Vault)** | Integrated storage | Continuous | 30 jours | Vault HA | Transit |

## Backup Verification â€” AutomatisÃ©e

| Check | Frequency | Automation | Alert si Ã©chec |
|-------|-----------|------------|----------------|
| PostgreSQL restore test | Weekly | Job K8s scheduled | P2 |
| Terraform state integrity | Daily | CI pipeline | P3 |
| Vault backup verification | Weekly | Job K8s scheduled | P2 |
| Git clone verification | Monthly | GitHub Actions | P4 |

## Backup Monitoring

| Metric | Alert Threshold | Severity |
|--------|-----------------|----------|
| Last backup age | > 2 hours | P2 |
| Backup size anomaly | > 50% change | P3 |
| Backup job failure | Any failure | P2 |
| PITR lag | > 1 hour | P2 |

---

# ğŸ¤– **Automated Recovery**

## Principe : Self-Healing Infrastructure

> **Objectif :** Minimiser l'intervention humaine. Le systÃ¨me doit se rÃ©parer automatiquement.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       AUTOMATED RECOVERY LAYERS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  LAYER 1: APPLICATION (Kubernetes)                                          â”‚
â”‚  â”œâ”€â”€ Liveness probes â†’ Restart automatique                                 â”‚
â”‚  â”œâ”€â”€ Readiness probes â†’ Traffic routing                                    â”‚
â”‚  â”œâ”€â”€ HPA â†’ Scale automatique                                               â”‚
â”‚  â””â”€â”€ PDB â†’ Protection pendant maintenance                                  â”‚
â”‚                                                                              â”‚
â”‚  LAYER 2: DATABASE (Aiven)                                                  â”‚
â”‚  â”œâ”€â”€ Health monitoring â†’ Failover automatique                              â”‚
â”‚  â”œâ”€â”€ Connection pooling (PgBouncer) â†’ Retry transparent                    â”‚
â”‚  â””â”€â”€ Read replicas â†’ Load distribution                                     â”‚
â”‚                                                                              â”‚
â”‚  LAYER 3: MESSAGING (Kafka)                                                 â”‚
â”‚  â”œâ”€â”€ Broker failure â†’ Partition rebalance automatique                      â”‚
â”‚  â”œâ”€â”€ Consumer failure â†’ Rebalance consumer group                           â”‚
â”‚  â””â”€â”€ Producer retry â†’ Idempotent delivery                                  â”‚
â”‚                                                                              â”‚
â”‚  LAYER 4: CACHE (Valkey)                                                    â”‚
â”‚  â”œâ”€â”€ Cache miss â†’ Fallback DB automatique (cache-aside pattern)           â”‚
â”‚  â”œâ”€â”€ Node failure â†’ Cluster failover                                       â”‚
â”‚  â””â”€â”€ TTL expiration â†’ Lazy refresh                                         â”‚
â”‚                                                                              â”‚
â”‚  LAYER 5: NETWORKING (Cloudflare + Cilium)                                  â”‚
â”‚  â”œâ”€â”€ Origin failure â†’ Health check + failover                              â”‚
â”‚  â”œâ”€â”€ DDoS â†’ Auto-mitigation                                                â”‚
â”‚  â””â”€â”€ mTLS â†’ Automatic certificate rotation                                 â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Recovery automatique par composant

| Composant | Failure | Recovery Mechanism | Temps | Intervention |
|-----------|---------|-------------------|-------|--------------|
| **Pod** | Crash | Kubernetes restart | < 30s | Aucune |
| **Pod** | OOM | Kubernetes restart + alert | < 30s | Investigation |
| **Deployment** | Bad deploy | ArgoCD rollback auto (si configurÃ©) | < 2min | Aucune |
| **DB Primary** | Failure | Aiven automatic failover | < 5min | Aucune |
| **DB Connection** | Pool exhausted | PgBouncer retry + scale | < 1min | Aucune |
| **Kafka Consumer** | Lag > threshold | KEDA auto-scale | < 2min | Aucune |
| **Cache** | Node down | Cluster failover + fallback DB | < 1min | Aucune |
| **Certificate** | Expiring | Cert-manager auto-renew | N/A | Aucune |

---

# ğŸ”¬ **Chaos Engineering**

## Philosophie

> **"Nous ne testons pas si le systÃ¨me tombe, mais si le systÃ¨me se relÃ¨ve."**

## Chaos Testing Framework

| Outil | Usage | IntÃ©gration |
|-------|-------|-------------|
| **Chaos Mesh** | Injection de pannes Kubernetes | CRD natif K8s |
| **Litmus** | Alternative open-source | Scenarios prÃ©dÃ©finis |
| **Gremlin** | Enterprise (si budget) | SaaS, plus de features |

## Experiments AutomatisÃ©s

| Experiment | Target | FrÃ©quence | Validation |
|------------|--------|-----------|------------|
| **Pod Kill** | Random pod in service | Daily (staging) | Service continues responding |
| **Network Latency** | Inter-service +100ms | Weekly | SLO latency maintained |
| **Node Drain** | Random node | Weekly | Pods rescheduled, no downtime |
| **DB Failover** | Force primary switch | Monthly | Connections recover < 5min |
| **Cache Flush** | Valkey flush | Weekly | Fallback to DB works |
| **AZ Failure Simulation** | Cordon all nodes in 1 AZ | Quarterly | Traffic shifts to other AZs |

## Chaos Test Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      CHAOS TESTING PIPELINE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                              â”‚
â”‚  1. PRE-CHECK                                                               â”‚
â”‚     â€¢ Verify system healthy (all green)                                     â”‚
â”‚     â€¢ Baseline metrics recorded                                             â”‚
â”‚     â€¢ Alerting team notified (staging)                                      â”‚
â”‚                                                                              â”‚
â”‚  2. INJECT CHAOS                                                            â”‚
â”‚     â€¢ Apply Chaos Mesh experiment                                           â”‚
â”‚     â€¢ Duration: 5-15 minutes                                                â”‚
â”‚                                                                              â”‚
â”‚  3. OBSERVE                                                                 â”‚
â”‚     â€¢ Monitor SLIs (error rate, latency)                                    â”‚
â”‚     â€¢ Check recovery mechanisms activate                                    â”‚
â”‚     â€¢ Record recovery time                                                  â”‚
â”‚                                                                              â”‚
â”‚  4. VALIDATE                                                                â”‚
â”‚     â€¢ SLO maintained? âœ… / âŒ                                               â”‚
â”‚     â€¢ Recovery time within RTO? âœ… / âŒ                                     â”‚
â”‚     â€¢ No data loss? âœ… / âŒ                                                 â”‚
â”‚                                                                              â”‚
â”‚  5. CLEANUP                                                                 â”‚
â”‚     â€¢ Remove chaos experiment                                               â”‚
â”‚     â€¢ Verify system back to baseline                                        â”‚
â”‚     â€¢ Generate report                                                       â”‚
â”‚                                                                              â”‚
â”‚  6. ACTION                                                                  â”‚
â”‚     â€¢ Si Ã©chec â†’ Ticket pour fix                                           â”‚
â”‚     â€¢ Si succÃ¨s â†’ Increase chaos scope                                     â”‚
â”‚                                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Game Days

| ActivitÃ© | FrÃ©quence | Participants | Scope |
|----------|-----------|--------------|-------|
| **Chaos Friday** | Weekly | On-call | Staging, experiments simples |
| **Game Day** | Monthly | Full team | Staging, multi-failure scenarios |
| **DR Drill** | Quarterly | Team + Management | Staging, full DR simulation |
| **Production Chaos** | Annually | Team + SRE | Prod (maintenance window) |

---

# ğŸ¥ **Disaster Recovery**

## DR Scenarios

| Scenario | Recovery | Automation Level |
|----------|----------|------------------|
| Single AZ failure | Automatique (multi-AZ) | 100% |
| Region failure | Semi-automatique (IaC + GitOps) | 80% |
| Data corruption | PITR restore | 60% |
| Ransomware | Immutable backups restore | 50% |

## DR Automation â€” Infrastructure as Code

> **Principe :** Toute l'infrastructure est reproductible via Terraform + ArgoCD.

| Composant | ReproductibilitÃ© | Temps estimÃ© |
|-----------|------------------|--------------|
| **EKS Cluster** | Terraform apply | ~30 min |
| **Platform tools** | ArgoCD sync | ~15 min |
| **Applications** | ArgoCD sync | ~10 min |
| **Database** | Aiven restore from backup | ~1-2h |
| **DNS cutover** | Cloudflare API / Terraform | ~5 min |

## DR Runbook â€” Region Failure

| Phase | DurÃ©e | Actions | Automation |
|-------|-------|---------|------------|
| **1. Detection** | 15 min | Confirmer failure, dÃ©clarer DR | Alerting automatique |
| **2. Infrastructure** | 1-2h | Terraform apply DR region | Semi-auto (approval required) |
| **3. Data** | 1-2h | Aiven restore, verify integrity | Semi-auto (Aiven console) |
| **4. Applications** | 30 min | ArgoCD sync | Automatique |
| **5. Traffic** | 15 min | Cloudflare DNS update | Semi-auto (Terraform) |
| **6. Validation** | 30 min | E2E tests, verify SLIs | Automatique (CI) |

**RTO Total : 4 heures**

## DR Test Schedule

| Test | FrÃ©quence | Scope | Duration |
|------|-----------|-------|----------|
| Backup restore | Weekly | PostgreSQL single table | 30 min |
| Failover test | Monthly | Database failover | 1 hour |
| DR drill | Quarterly | Full DR simulation (staging) | 4 hours |
| Full DR test | Annually | Production DR (maintenance) | 8 hours |

---

# ğŸ“Š **Business Continuity**

## RPO/RTO Summary

| Scenario | RPO | RTO | Data Loss Risk | Automation |
|----------|-----|-----|----------------|------------|
| Pod failure | 0 | < 30s | None | 100% auto |
| Node failure | 0 | < 2min | None | 100% auto |
| AZ failure | 0 | < 5min | None | 100% auto |
| DB failover | 0 (sync) | < 5min | None | 100% auto |
| Region failure | 1 hour | 4 hours | Up to 1 hour | 80% auto |

## Communication Plan

| Audience | Channel | Frequency | Owner |
|----------|---------|-----------|-------|
| Engineering | Slack #incidents | Real-time | On-call |
| Management | Email + Slack | Every 30 min | Incident Commander |
| Customers | Status page | Every 15 min | Communications |
| Partners | Email | Major updates | Account Management |

## Status Page

| Status | Definition |
|--------|------------|
| **Operational** | All systems normal |
| **Degraded** | Partial impact (increased latency) |
| **Partial Outage** | Some features unavailable |
| **Major Outage** | Service unavailable |

---

## Recovery Validation â€” AutomatisÃ©e

> Ces checks sont exÃ©cutÃ©s automatiquement aprÃ¨s chaque recovery.

### Health Check Pipeline

| Check | Method | Failure Action |
|-------|--------|----------------|
| All pods running | kubectl health check | Alert P1 |
| Metrics flowing | Prometheus query | Alert P2 |
| Logs flowing | Loki query | Alert P2 |
| Traces flowing | Tempo query | Alert P3 |
| E2E critical path | Automated tests | Alert P1 |
| Error rate normal | SLI check | Alert P2 |

### Database Validation

| Check | Method | Failure Action |
|-------|--------|----------------|
| Data integrity | Checksum validation | Alert P1 |
| Transaction count | Count comparison | Alert P2 |
| FK constraints | DB validation | Alert P2 |
| Read/Write test | Smoke test | Alert P1 |

---

> **Voir aussi :** [Testing Strategy](../testing/TESTING-STRATEGY.md) pour les tests de performance, load testing, et chaos engineering dÃ©taillÃ©s.

---

*Document maintenu par : Platform Team + SRE*  
*DerniÃ¨re mise Ã  jour : Janvier 2026*
